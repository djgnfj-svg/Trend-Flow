"""
íŠ¸ë Œë“œ ìˆ˜ì§‘ ë° ë¶„ì„ íŒŒì´í”„ë¼ì¸ DAG
ë§¤ì¼ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ GitHub Trendingì„ ìˆ˜ì§‘í•˜ê³  AIë¡œ ë¶„ì„
"""
from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
from pathlib import Path

# DAG ê²½ë¡œë¥¼ Python ê²½ë¡œì— ì¶”ê°€
dag_folder = Path(__file__).parent
sys.path.insert(0, str(dag_folder))

from collectors.github_trending import scrape_github_trending
from database.db_manager import DatabaseManager
from ai_analysis.analyzer import TrendAnalyzer


def collect_github_trending(**context):
    """GitHub Trending ìˆ˜ì§‘ íƒœìŠ¤í¬"""
    print("=" * 60)
    print("ğŸ” GitHub Trending ìˆ˜ì§‘ ì‹œì‘")
    print("=" * 60)

    try:
        # íŠ¸ë Œë”© ë ˆí¬ì§€í† ë¦¬ í¬ë¡¤ë§ (ìƒìœ„ 25ê°œ)
        trends = scrape_github_trending(language="", limit=25)

        if not trends:
            print("âš ï¸  ìˆ˜ì§‘ëœ íŠ¸ë Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        db = DatabaseManager()
        saved_count = db.save_trends('github_trending', trends)

        print(f"\nâœ… GitHub Trending ìˆ˜ì§‘ ì™„ë£Œ: {saved_count}ê°œ ì €ì¥")

        # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
        context['task_instance'].xcom_push(key='collected_count', value=saved_count)

    except Exception as e:
        print(f"âŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        raise


def analyze_trends(**context):
    """ìˆ˜ì§‘ëœ íŠ¸ë Œë“œ AI ë¶„ì„ íƒœìŠ¤í¬"""
    print("=" * 60)
    print("ğŸ¤– íŠ¸ë Œë“œ AI ë¶„ì„ ì‹œì‘")
    print("=" * 60)

    try:
        db = DatabaseManager()
        analyzer = TrendAnalyzer(ollama_host='http://ollama:11434')

        # ë¶„ì„ë˜ì§€ ì•Šì€ íŠ¸ë Œë“œ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 10ê°œ)
        unanalyzed = db.get_unanalyzed_trends(limit=10)

        if not unanalyzed:
            print("âš ï¸  ë¶„ì„í•  íŠ¸ë Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            context['task_instance'].xcom_push(key='analyzed_count', value=0)
            return

        print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ: {len(unanalyzed)}ê°œ íŠ¸ë Œë“œ")

        analyzed_count = 0
        solution_count = 0

        for trend in unanalyzed:
            print(f"\n{'=' * 60}")
            print(f"ë¶„ì„ ì¤‘: {trend['title']}")
            print(f"{'=' * 60}")

            # 1. íŠ¸ë Œë“œ ë¶„ì„
            analysis = analyzer.analyze_trend(trend)

            if not analysis:
                print(f"âš ï¸  ë¶„ì„ ìŠ¤í‚µ: {trend['title']}")
                continue

            # 2. ë¶„ì„ ê²°ê³¼ ì €ì¥
            try:
                analyzed_id = db.save_analysis(trend['id'], analysis)
                analyzed_count += 1
                print(f"âœ… ë¶„ì„ ì €ì¥ ì™„ë£Œ (ID: {analyzed_id})")

                # 3. ì†”ë£¨ì…˜ ìƒì„±
                solutions = analyzer.generate_solutions(trend, analysis)

                if solutions:
                    # 4. ì†”ë£¨ì…˜ ì €ì¥
                    saved_solutions = db.save_solutions(analyzed_id, solutions)
                    solution_count += saved_solutions
                    print(f"âœ… ì†”ë£¨ì…˜ {saved_solutions}ê°œ ì €ì¥")

            except Exception as e:
                print(f"âš ï¸  ì €ì¥ ì˜¤ë¥˜ ({trend['title']}): {str(e)}")
                continue

        print(f"\n{'=' * 60}")
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {analyzed_count}ê°œ ë¶„ì„, {solution_count}ê°œ ì†”ë£¨ì…˜ ìƒì„±")
        print(f"{'=' * 60}")

        # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
        context['task_instance'].xcom_push(key='analyzed_count', value=analyzed_count)
        context['task_instance'].xcom_push(key='solution_count', value=solution_count)

    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        raise


def print_daily_summary(**context):
    """ì¼ì¼ ìš”ì•½ ì¶œë ¥ íƒœìŠ¤í¬"""
    print("=" * 60)
    print("ğŸ“Š ì˜¤ëŠ˜ì˜ íŠ¸ë Œë“œ ìˆ˜ì§‘/ë¶„ì„ ìš”ì•½")
    print("=" * 60)

    try:
        db = DatabaseManager()
        stats = db.get_today_stats()

        print(f"\nğŸ“ˆ ì˜¤ëŠ˜ì˜ í†µê³„:")
        print(f"  â€¢ ìˆ˜ì§‘ëœ íŠ¸ë Œë“œ: {stats.get('total_trends', 0)}ê°œ")
        print(f"  â€¢ ë¶„ì„ ì™„ë£Œ: {stats.get('analyzed_trends', 0)}ê°œ")
        print(f"  â€¢ ìƒì„±ëœ ì†”ë£¨ì…˜: {stats.get('total_solutions', 0)}ê°œ")

        avg_importance = stats.get('avg_importance')
        if avg_importance:
            print(f"  â€¢ í‰ê·  ì¤‘ìš”ë„: {float(avg_importance):.2f}/10")

        # XComì—ì„œ ì´ë²ˆ ì‹¤í–‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        ti = context['task_instance']
        collected = ti.xcom_pull(task_ids='collect_github_trending', key='collected_count')
        analyzed = ti.xcom_pull(task_ids='analyze_trends', key='analyzed_count')
        solutions = ti.xcom_pull(task_ids='analyze_trends', key='solution_count')

        print(f"\nğŸ¯ ì´ë²ˆ ì‹¤í–‰ ê²°ê³¼:")
        print(f"  â€¢ ìƒˆë¡œ ìˆ˜ì§‘: {collected or 0}ê°œ")
        print(f"  â€¢ ìƒˆë¡œ ë¶„ì„: {analyzed or 0}ê°œ")
        print(f"  â€¢ ìƒˆ ì†”ë£¨ì…˜: {solutions or 0}ê°œ")

        print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
        print("=" * 60)

    except Exception as e:
        print(f"âŒ ìš”ì•½ ì¶œë ¥ ì‹¤íŒ¨: {str(e)}")
        raise


# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# DAG ì •ì˜
with DAG(
    'trend_collection_and_analysis',
    default_args=default_args,
    description='GitHub Trending ìˆ˜ì§‘ ë° AI ë¶„ì„ íŒŒì´í”„ë¼ì¸',
    schedule='0 9 * * *',  # ë§¤ì¼ ì˜¤ì „ 9ì‹œ ì‹¤í–‰
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['trend', 'ai', 'github'],
) as dag:

    # Task 1: GitHub Trending ìˆ˜ì§‘
    collect_task = PythonOperator(
        task_id='collect_github_trending',
        python_callable=collect_github_trending,
    )

    # Task 2: AI ë¶„ì„ ë° ì†”ë£¨ì…˜ ìƒì„±
    analyze_task = PythonOperator(
        task_id='analyze_trends',
        python_callable=analyze_trends,
    )

    # Task 3: ì¼ì¼ ìš”ì•½ ì¶œë ¥
    summary_task = PythonOperator(
        task_id='print_daily_summary',
        python_callable=print_daily_summary,
    )

    # Task ì˜ì¡´ì„± ì •ì˜
    collect_task >> analyze_task >> summary_task

# Trend-Flow 프로젝트 체크리스트

## 프로젝트 목표
세상의 트렌드를 분석하여 문제를 해결할 솔루션 아이디어를 제공하는 시스템

---

## Phase 1: 크롤링 → 로컬 AI 분석 → DB 저장

### 1. 로컬 AI 모델 설정
- [ ] 로컬 AI 모델 선택
  - [ ] Ollama 설치 및 설정 (추천: llama3, mistral 등)
  - [ ] 또는 LM Studio 설정
  - [ ] 또는 GPT4All 설정
- [ ] Docker Compose에 Ollama 컨테이너 추가
- [ ] AI 모델 API 연동 테스트

### 2. 데이터베이스 스키마 설계
- [ ] 테이블 설계
  - [ ] `trends` - 수집한 트렌드 원본 데이터
  - [ ] `analyzed_trends` - AI 분석 결과
  - [ ] `solutions` - 솔루션 아이디어
  - [ ] `sources` - 트렌드 출처 관리
- [ ] PostgreSQL 마이그레이션 스크립트 작성
- [ ] 테이블 생성 및 테스트

### 3. 트렌드 크롤링 구현
- [ ] 크롤링 대상 사이트 선정
  - [ ] 네이버 트렌드
  - [ ] Google Trends
  - [ ] GitHub Trending
  - [ ] Product Hunt
  - [ ] 기타: ___________
- [ ] 각 사이트별 크롤링 함수 작성
- [ ] 크롤링 데이터 정규화 (제목, 설명, 링크, 카테고리 등)
- [ ] 에러 핸들링 및 재시도 로직 구현

### 4. AI 분석 파이프라인
- [ ] 프롬프트 엔지니어링
  - [ ] 트렌드 요약 프롬프트
  - [ ] 문제점 추출 프롬프트
  - [ ] 솔루션 아이디어 생성 프롬프트
- [ ] AI 분석 함수 작성
- [ ] 분석 결과 파싱 및 구조화

### 5. Airflow DAG 작성
- [ ] `trend_collection_dag.py` 생성
  - [ ] Task 1: 트렌드 크롤링
  - [ ] Task 2: 로컬 AI 분석
  - [ ] Task 3: DB 저장
  - [ ] Task 4: 알림 (Slack 등)
- [ ] DAG 스케줄 설정 (매일 아침 8시 등)
- [ ] 태스크 간 의존성 설정
- [ ] 에러 알림 설정

### 6. 테스트 및 검증
- [ ] 크롤링 단위 테스트
- [ ] AI 분석 단위 테스트
- [ ] DB 저장 테스트
- [ ] 전체 DAG 통합 테스트
- [ ] 데이터 품질 검증

---

## Phase 2: 대시보드 및 알림 (향후)

### 7. 데이터 시각화
- [ ] 대시보드 도구 선택 (Grafana, Streamlit 등)
- [ ] 트렌드 통계 시각화
- [ ] 솔루션 아이디어 목록 표시

### 8. 알림 시스템
- [ ] 매일 아침 트렌드 요약 슬랙 알림
- [ ] 주간 리포트 이메일 발송
- [ ] 특정 키워드 트렌드 감지 시 알림

---

## Phase 3: 고도화 (향후)

### 9. 데이터 분석 강화
- [ ] 트렌드 카테고리 자동 분류
- [ ] 유사 트렌드 군집화
- [ ] 트렌드 중요도 점수화
- [ ] 시계열 분석 (상승/하락 트렌드)

### 10. 솔루션 아이디어 개선
- [ ] 실행 가능성 평가
- [ ] 시장 규모 예측
- [ ] 경쟁사 분석
- [ ] 비즈니스 모델 제안

---

## 기술 스택

### 현재 설정 완료
- ✅ Apache Airflow 3.1.2
- ✅ PostgreSQL 16
- ✅ Docker & Docker Compose
- ✅ Python 라이브러리 (requests, beautifulsoup4, selenium, pandas)

### 추가 필요
- [ ] Ollama (로컬 AI)
- [ ] SQLAlchemy (ORM)
- [ ] Alembic (DB 마이그레이션)
- [ ] 대시보드 도구 (선택사항)

---

## 다음 단계 우선순위

1. **로컬 AI 모델 설정** - Ollama 추천
2. **데이터베이스 스키마 설계** - 테이블 구조 정의
3. **트렌드 크롤링 사이트 1개 선정** - 네이버 트렌드 추천
4. **간단한 DAG 작성** - 크롤링 → AI 분석 → DB 저장
5. **테스트 및 검증**

---

## 참고사항

- 로컬 AI는 Ollama를 사용하면 Docker로 쉽게 설정 가능
- 크롤링 시 로봇 차단 방지를 위해 User-Agent 설정 필수
- AI 분석 결과는 JSON 형식으로 구조화하여 저장
- 매일 실행 시간은 트래픽이 적은 새벽/아침 시간대 추천
